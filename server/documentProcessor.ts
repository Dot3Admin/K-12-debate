import { spawn, exec } from 'child_process';
import path from 'path';
import fs from 'fs';
import { fileURLToPath } from 'url';
import { dirname } from 'path';
import { db } from './db';
import { agentDocumentChunks, documents } from '../shared/schema';
import { eq, inArray, and, or, gt, isNull } from 'drizzle-orm';
import { analyzeDocument, analyzePDFPageImage } from './openai';
import { storage } from './storage';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

interface ProcessedDocument {
  text: string;
  tables: any[];
  images: any[];
  formulas: any[];
  metadata: any;
}

interface RAGChunk {
  text: string;
  chunk_index: number;
  char_count: number;
  word_count: number;
  keywords: string[];
  metadata: any;
}

export async function processDocument(filePath: string, documentId: number, agentId: number, originalName?: string): Promise<{ success: boolean; chunks?: number; text?: string; analysis?: any; error?: string }> {
  try {
    const startTime = Date.now();
    console.log(`\n${'='.repeat(80)}`);
    console.log(`ğŸ“„ [ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘] ${originalName || path.basename(filePath)}`);
    console.log(`${'='.repeat(80)}\n`);
    
    // Step 1: Extract text and metadata from document
    console.log(`[1/4] ğŸ“– í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...`);
    console.log(`  - íŒŒì¼ ê²½ë¡œ: ${filePath}`);
    console.log(`  - ì›ë³¸ íŒŒì¼ëª…: ${originalName || 'Unknown'}`);
    const fileSize = fs.existsSync(filePath) ? fs.statSync(filePath).size : 0;
    console.log(`  - íŒŒì¼ í¬ê¸°: ${(fileSize / 1024).toFixed(2)} KB`);
    console.log(`  - ì˜ˆìƒ ì†Œìš” ì‹œê°„: ${fileSize > 1024 * 1024 ? '10-30ì´ˆ' : '5-15ì´ˆ'}`);
    
    const extractStart = Date.now();
    const processedDoc = await extractDocumentContent(filePath, originalName);
    const extractDuration = ((Date.now() - extractStart) / 1000).toFixed(1);
    
    if (!processedDoc || !processedDoc.text) {
      throw new Error('No text content extracted from document');
    }
    
    console.log(`\n  âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (${extractDuration}ì´ˆ)`);
    console.log(`  - ì¶”ì¶œëœ ë¬¸ì ìˆ˜: ${processedDoc.text.length.toLocaleString()}ì`);
    console.log(`  - ë‹¨ì–´ ìˆ˜ (ì¶”ì •): ${Math.round(processedDoc.text.split(/\s+/).length).toLocaleString()}ê°œ`);
    if (processedDoc.metadata?.total_pages) {
      console.log(`  - ì´ í˜ì´ì§€ ìˆ˜: ${processedDoc.metadata.total_pages}í˜ì´ì§€`);
    }
    if (processedDoc.metadata?.ocr_used) {
      console.log(`  - ğŸ–¼ï¸  OCR ì‚¬ìš©ë¨: ${processedDoc.metadata.ocr_pages}ê°œ í˜ì´ì§€`);
    }
    
    // Step 2: Analyze document with OpenAI (Text + Vision API if enabled)
    console.log(`\n[2/4] ğŸ¤– OpenAI ë¶„ì„ ì‹œì‘...`);
    console.log(`  - ë¶„ì„ ëŒ€ìƒ: ${processedDoc.text.length.toLocaleString()}ì`);
    console.log(`  - ì˜ˆìƒ í† í° ìˆ˜: ~${Math.round(processedDoc.text.length / 3).toLocaleString()} í† í°`);
    
    const analysisStart = Date.now();
    const analysis = await analyzeDocument(processedDoc.text, originalName || 'document');
    const analysisDuration = ((Date.now() - analysisStart) / 1000).toFixed(1);
    
    console.log(`\n  âœ… OpenAI í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ (${analysisDuration}ì´ˆ)`);
    if (analysis.summary) {
      console.log(`  - ìš”ì•½ ê¸¸ì´: ${analysis.summary.length}ì`);
      console.log(`  - ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°: ${analysis.summary.substring(0, 80)}...`);
    } else {
      console.log(`  - âš ï¸  ìš”ì•½ ìƒì„± ì‹¤íŒ¨`);
    }
    if (analysis.keyPoints?.length) {
      console.log(`  - í•µì‹¬ í¬ì¸íŠ¸: ${analysis.keyPoints.length}ê°œ ì¶”ì¶œë¨`);
    }
    
    // Analyze document structure for Vision API recommendation (no automatic execution)
    const isPDF = (originalName || filePath).toLowerCase().endsWith('.pdf');
    console.log(`\n  ğŸ” ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì¤‘... (ë‹¤ì´ì–´ê·¸ë¨ ê°ì§€)`);
    const structureStart = Date.now();
    const visionAnalysis = await analyzeDocumentStructure(processedDoc.text, processedDoc.metadata);
    const structureDuration = ((Date.now() - structureStart) / 1000).toFixed(1);
    console.log(`  âœ… êµ¬ì¡° ë¶„ì„ ì™„ë£Œ (${structureDuration}ì´ˆ)`);
    
    if (visionAnalysis.recommendVision) {
      console.log(`  ğŸ“Š Vision API ê¶Œì¥: ë‹¤ì´ì–´ê·¸ë¨ ${visionAnalysis.diagramCount}ê°œ ê°ì§€`);
      console.log(`  ğŸ’° ì˜ˆìƒ ë¹„ìš©: $${visionAnalysis.estimatedCost?.toFixed(4)} (Sharp ìµœì í™” ì ìš©ì‹œ)`);
    } else {
      console.log(`  â„¹ï¸  Vision API ë¹„ê¶Œì¥: í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¬¸ì„œ`);
    }
    
    // Step 3: Update document with analysis results (if documentId exists)
    if (documentId > 0) {
      console.log(`\n[3/4] ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸...`);
      await db.update(documents)
        .set({
          description: analysis.summary,
          visionAnalysis: visionAnalysis
        })
        .where(eq(documents.id, documentId));
      console.log(`  âœ… ë¬¸ì„œ ${documentId} ì—…ë°ì´íŠ¸ ì™„ë£Œ`);
    }
    
    // Step 4: Generate RAG chunks (only if documentId > 0)
    if (documentId > 0) {
      console.log(`\n[4/4] ğŸ” RAG ì²­í¬ ìƒì„± ì‹œì‘...`);
      console.log(`  - ì²­í¬ ì „ëµ: semantic (ì˜ë¯¸ë¡ ì  ë¶„í• )`);
      console.log(`  - ì²­í¬ í¬ê¸° ë²”ìœ„: 200-800ì`);
      console.log(`  - ì˜ˆìƒ ì²­í¬ ìˆ˜: ~${Math.ceil(processedDoc.text.length / 500)}ê°œ (í‰ê·  500ì ê¸°ì¤€)`);
      
      const chunkStart = Date.now();
      const chunks = await generateRAGChunks(processedDoc);
      const chunkDuration = ((Date.now() - chunkStart) / 1000).toFixed(1);
      
      if (chunks.length === 0) {
        console.error(`\n  âŒ RAG ì²­í¬ ìƒì„± ì‹¤íŒ¨: ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤`);
        throw new Error('RAG chunking failed: no chunks generated');
      }
      
      console.log(`\n  âœ… RAG ì²­í¬ ìƒì„± ì™„ë£Œ (${chunkDuration}ì´ˆ)`);
      console.log(`  - ìƒì„±ëœ ì²­í¬ ìˆ˜: ${chunks.length}ê°œ`);
      console.log(`  - í‰ê·  ì²­í¬ í¬ê¸°: ${Math.round(processedDoc.text.length / chunks.length)}ì`);
      console.log(`  - ì´ ì»¤ë²„ë¦¬ì§€: ${processedDoc.text.length.toLocaleString()}ì`);
      
      // Add summary and metadata chunks for better search
      console.log(`\n  ğŸ“ ìš”ì•½/ë©”íƒ€ë°ì´í„° ì²­í¬ ì¶”ê°€ ì¤‘...`);
      const metadataChunks: RAGChunk[] = [];
      
      // Add summary chunk if available
      if (analysis.summary) {
        metadataChunks.push({
          text: `[ë¬¸ì„œ ìš”ì•½: ${originalName || 'document'}]\n${analysis.summary}`,
          chunk_index: -1, // Special index for summary
          char_count: analysis.summary.length,
          word_count: analysis.summary.split(/\s+/).length,
          keywords: ['ìš”ì•½', 'summary', originalName || 'document'].concat(
            analysis.keyPoints?.slice(0, 5) || []
          ),
          metadata: { type: 'summary', filename: originalName }
        });
        console.log(`  - ìš”ì•½ ì²­í¬ ì¶”ê°€ ì™„ë£Œ`);
      }
      
      // Add metadata chunk with filename and description
      if (originalName) {
        const metadataText = `[íŒŒì¼ ì •ë³´]\níŒŒì¼ëª…: ${originalName}\n` + 
          (analysis.summary ? `ì„¤ëª…: ${analysis.summary.substring(0, 200)}...` : '');
        metadataChunks.push({
          text: metadataText,
          chunk_index: -2, // Special index for metadata
          char_count: metadataText.length,
          word_count: metadataText.split(/\s+/).length,
          keywords: [originalName, 'íŒŒì¼', 'file', 'document'],
          metadata: { type: 'metadata', filename: originalName }
        });
        console.log(`  - ë©”íƒ€ë°ì´í„° ì²­í¬ ì¶”ê°€ ì™„ë£Œ`);
      }
      
      const allChunks = [...metadataChunks, ...chunks];
      console.log(`  âœ… ì´ ${allChunks.length}ê°œ ì²­í¬ (ë©”íƒ€: ${metadataChunks.length}, ë³¸ë¬¸: ${chunks.length})`);
      
      // Step 5: Save chunks to database with embeddings
      console.log(`\n  ğŸ’¾ ì²­í¬ ì €ì¥ ë° embedding ìƒì„± ì¤‘...`);
      await saveChunksToDatabase(allChunks, documentId, agentId, originalName);
      console.log(`  âœ… ${allChunks.length}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ`);
      
      const totalDuration = ((Date.now() - startTime) / 1000).toFixed(1);
      console.log(`\n${'='.repeat(80)}`);
      console.log(`âœ… [ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ] ì´ ì†Œìš” ì‹œê°„: ${totalDuration}ì´ˆ`);
      console.log(`${'='.repeat(80)}\n`);
      
      return {
        success: true,
        chunks: chunks.length,
        text: processedDoc.text,
        analysis
      };
    } else {
      // If documentId is 0, just return text and analysis without saving
      const totalDuration = ((Date.now() - startTime) / 1000).toFixed(1);
      console.log(`\n${'='.repeat(80)}`);
      console.log(`âœ… [ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ] ì´ ì†Œìš” ì‹œê°„: ${totalDuration}ì´ˆ (ì €ì¥ ìƒëµ)`);
      console.log(`${'='.repeat(80)}\n`);
      
      return {
        success: true,
        text: processedDoc.text,
        analysis
      };
    }
    
  } catch (error) {
    console.error(`\n${'='.repeat(80)}`);
    console.error('âŒ [ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨]');
    console.error(`${'='.repeat(80)}`);
    console.error('[Document Processor] Error:', error);
    return {
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error'
    };
  }
}

async function extractDocumentContent(filePath: string, originalName?: string): Promise<ProcessedDocument> {
  return new Promise((resolve, reject) => {
    const pythonScript = path.join(__dirname, 'document_processor', 'extract_document.py');
    
    // Create the Python script if it doesn't exist
    const scriptDir = path.dirname(pythonScript);
    if (!fs.existsSync(scriptDir)) {
      fs.mkdirSync(scriptDir, { recursive: true });
    }
    
    // Create a simple Python script that uses the processors
    const extractScript = `
import sys
import json
import os
sys.path.insert(0, os.path.dirname(__file__))

from processors import get_processor

def main():
    if len(sys.argv) < 2:
        print(json.dumps({"error": "No file path provided"}))
        sys.exit(1)
    
    file_path = sys.argv[1]
    original_name = sys.argv[2] if len(sys.argv) > 2 else None
    
    try:
        processor = get_processor(file_path, original_name)
        result = processor.process(file_path)
        print(json.dumps(result, ensure_ascii=False))
    except Exception as e:
        print(json.dumps({"error": str(e)}))
        sys.exit(1)

if __name__ == "__main__":
    main()
`;
    
    fs.writeFileSync(pythonScript, extractScript);
    
    // Pass original filename if available to help with file type detection
    const args = [pythonScript, filePath];
    if (originalName) {
      args.push(originalName);
    }
    
    const python = spawn('python3', args, {
      env: { ...process.env }
    });
    
    let stdout = '';
    let stderr = '';
    
    python.stdout.on('data', (data) => {
      stdout += data.toString();
    });
    
    python.stderr.on('data', (data) => {
      stderr += data.toString();
    });
    
    python.on('close', (code) => {
      if (code !== 0) {
        console.error('[Python] stderr:', stderr);
        reject(new Error(`Python process exited with code ${code}: ${stderr}`));
        return;
      }
      
      try {
        const result = JSON.parse(stdout);
        
        if (result.error) {
          reject(new Error(result.error));
          return;
        }
        
        resolve(result as ProcessedDocument);
      } catch (error) {
        reject(new Error(`Failed to parse Python output: ${error}`));
      }
    });
  });
}

async function generateRAGChunks(processedDoc: ProcessedDocument): Promise<RAGChunk[]> {
  return new Promise((resolve, reject) => {
    const pythonScript = path.join(__dirname, 'document_processor', 'generate_chunks.py');
    
    // Create the RAG generation script - reads from stdin to avoid arg size limits
    const ragScript = `
import sys
import json
import os
sys.path.insert(0, os.path.dirname(__file__))

from generators.rag_generator import RAGGenerator

def main():
    try:
        # Read from stdin instead of command-line args
        input_data = sys.stdin.read()
        processed_doc = json.loads(input_data)
        
        generator = RAGGenerator(chunking_strategy='semantic', min_chunk_size=200, max_chunk_size=500)
        chunks = generator.generate_chunks(processed_doc)
        print(json.dumps(chunks, ensure_ascii=False))
    except Exception as e:
        print(json.dumps({"error": str(e)}), file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
`;
    
    fs.writeFileSync(pythonScript, ragScript);
    
    // Spawn without args, will pipe to stdin
    const python = spawn('python3', [pythonScript], {
      env: { ...process.env }
    });
    
    let stdout = '';
    let stderr = '';
    
    // Pipe the JSON to stdin to avoid OS arg size limits
    python.stdin.write(JSON.stringify(processedDoc));
    python.stdin.end();
    
    python.stdout.on('data', (data) => {
      stdout += data.toString();
    });
    
    python.stderr.on('data', (data) => {
      stderr += data.toString();
    });
    
    python.on('close', (code) => {
      if (code !== 0) {
        console.error('[Python RAG] stderr:', stderr);
        reject(new Error(`Python RAG process exited with code ${code}: ${stderr}`));
        return;
      }
      
      try {
        const chunks = JSON.parse(stdout);
        
        if (chunks.error) {
          reject(new Error(chunks.error));
          return;
        }
        
        resolve(chunks as RAGChunk[]);
      } catch (error) {
        reject(new Error(`Failed to parse Python RAG output: ${error}`));
      }
    });
  });
}

// Vision API: PDF ì‹œê°ì  ì½˜í…ì¸  ë¶„ì„
export async function analyzeVisualContent(
  pdfPath: string, 
  originalName?: string,
  options: {
    userId?: string;
    agentId?: number;
    documentId?: number;
  } = {}
): Promise<string | null> {
  return new Promise((resolve, reject) => {
    const pythonScript = path.join(__dirname, 'document_processor', 'pdf_to_image.py');
    
    // PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸ (PyMuPDF ì‚¬ìš© - ì´ë¯¸ ì„¤ì¹˜ë¨)
    const pdfToImageScript = `
import sys
import json
import os
import fitz  # PyMuPDF

def main():
    if len(sys.argv) < 2:
        print(json.dumps({"error": "No PDF path provided"}))
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    
    try:
        # PyMuPDFë¡œ PDF ì—´ê¸°
        doc = fitz.open(pdf_path)
        
        if len(doc) == 0:
            print(json.dumps({"error": "PDF has no pages"}))
            sys.exit(1)
        
        # ì²« í˜ì´ì§€ë§Œ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ë¹„ìš© ì ˆê°)
        page = doc[0]
        
        # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë Œë”ë§ (150 DPI)
        zoom = 150 / 72  # 72 DPIê°€ ê¸°ë³¸ê°’
        mat = fitz.Matrix(zoom, zoom)
        pix = page.get_pixmap(matrix=mat)
        
        # ì„ì‹œ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥
        temp_dir = os.path.dirname(pdf_path)
        image_path = os.path.join(temp_dir, "temp_vision_page_1.png")
        pix.save(image_path)
        
        doc.close()
        
        print(json.dumps({"success": True, "image_path": image_path}))
        
    except Exception as e:
        print(json.dumps({"error": str(e)}))
        sys.exit(1)

if __name__ == "__main__":
    main()
`;
    
    fs.writeFileSync(pythonScript, pdfToImageScript);
    
    const python = spawn('python3', [pythonScript, pdfPath]);
    
    let stdout = '';
    let stderr = '';
    
    python.stdout.on('data', (data) => {
      stdout += data.toString();
    });
    
    python.stderr.on('data', (data) => {
      stderr += data.toString();
    });
    
    python.on('close', async (code) => {
      if (code !== 0) {
        console.error('[PDF to Image] stderr:', stderr);
        reject(new Error(`PDF to image conversion failed: ${stderr}`));
        return;
      }
      
      try {
        const result = JSON.parse(stdout);
        
        if (result.error) {
          reject(new Error(result.error));
          return;
        }
        
        // Vision APIë¡œ ì´ë¯¸ì§€ ë¶„ì„
        const imagePath = result.image_path;
        
        try {
          // ë…¸ì„ ë„/ì§€ë„ë¡œ ì¶”ì •í•˜ì—¬ ë¶„ì„
          const visionAnalysis = await analyzePDFPageImage(imagePath, 1, "map", {
            userId: options.userId,
            agentId: options.agentId,
            documentId: options.documentId
          });
          
          // ì„ì‹œ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
          try {
            fs.unlinkSync(imagePath);
          } catch (cleanupError) {
            console.error('[Vision API] Failed to cleanup temp image:', cleanupError);
          }
          
          resolve(visionAnalysis);
        } catch (visionError) {
          // Vision API ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
          try {
            fs.unlinkSync(imagePath);
          } catch (cleanupError) {
            // Ignore cleanup errors
          }
          reject(visionError);
        }
        
      } catch (error) {
        reject(new Error(`Failed to parse PDF to image output: ${error}`));
      }
    });
  });
}

// Vision API: PPT/PPTX ì‹œê°ì  ì½˜í…ì¸  ë¶„ì„
export async function analyzePPTXVisualContent(
  pptPath: string,
  originalName?: string,
  options: {
    userId?: string;
    agentId?: number;
    documentId?: number;
  } = {}
): Promise<string | null> {
  return new Promise((resolve, reject) => {
    const fileExtension = path.extname(originalName || '').toLowerCase();
    console.log(`[Vision PPT] Converting ${fileExtension} to images: ${originalName}`);
    
    // PPT/PPTX â†’ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ (LibreOffice ì‚¬ìš©)
    const tempDir = path.dirname(pptPath);
    
    // Step 1: PPT/PPTX â†’ PDF (LibreOffice)
    // LibreOffice supports both .ppt and .pptx formats
    const libreofficeCmd = `libreoffice --headless --convert-to pdf --outdir "${tempDir}" "${pptPath}"`;
    
    let execProcess: any;
    let hasTimedOut = false;
    
    // Timeout for LibreOffice conversion (30 seconds)
    const conversionTimeout = setTimeout(() => {
      hasTimedOut = true;
      console.error('[Vision PPT] LibreOffice conversion timeout');
      if (execProcess) {
        execProcess.kill('SIGTERM');
      }
      reject(new Error('LibreOffice conversion timeout (30s). Please try again.'));
    }, 30000);
    
    execProcess = exec(libreofficeCmd, async (error, stdout, stderr) => {
      clearTimeout(conversionTimeout);
      
      // If already timed out, ignore the callback
      if (hasTimedOut) {
        return;
      }
      
      if (error) {
        console.error('[Vision PPT] LibreOffice conversion failed:', error);
        reject(new Error(`PPT/PPTX to PDF conversion failed: ${error.message}. LibreOffice may not be available.`));
        return;
      }
      
      console.log('[Vision PPT] PPT/PPTX â†’ PDF conversion successful');
      
      // Step 2: PDF â†’ ì´ë¯¸ì§€ (pdf2image Python)
      const pythonScript = path.join(__dirname, 'document_processor', 'pdf_to_image_ppt.py');
      
      const pdfToImageScript = `
import sys
import json
import os
from pdf2image import convert_from_path

def main():
    if len(sys.argv) < 2:
        print(json.dumps({"error": "No PDF path provided"}))
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    
    try:
        # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ì²« í˜ì´ì§€ë§Œ)
        images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=1)
        
        if len(images) == 0:
            print(json.dumps({"error": "No pages found"}))
            sys.exit(1)
        
        # ì„ì‹œ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥
        temp_dir = os.path.dirname(pdf_path)
        image_path = os.path.join(temp_dir, "temp_vision_ppt_slide_1.png")
        images[0].save(image_path, 'PNG')
        
        print(json.dumps({"success": True, "image_path": image_path}))
        
    except Exception as e:
        print(json.dumps({"error": str(e)}))
        sys.exit(1)

if __name__ == "__main__":
    main()
`;
      
      fs.writeFileSync(pythonScript, pdfToImageScript);
      
      // Find the converted PDF file (LibreOffice names it based on original file)
      const baseFilename = path.basename(pptPath, path.extname(pptPath));
      const convertedPdfPath = path.join(tempDir, `${baseFilename}.pdf`);
      
      if (!fs.existsSync(convertedPdfPath)) {
        reject(new Error('LibreOffice conversion succeeded but output PDF not found. Check file permissions.'));
        return;
      }
      
      const python = spawn('python3', [pythonScript, convertedPdfPath]);
      
      let pythonStdout = '';
      let pythonStderr = '';
      
      python.stdout.on('data', (data) => {
        pythonStdout += data.toString();
      });
      
      python.stderr.on('data', (data) => {
        pythonStderr += data.toString();
      });
      
      python.on('close', async (code) => {
        // Cleanup converted PDF
        try {
          fs.unlinkSync(convertedPdfPath);
        } catch (e) {
          console.error('[Vision PPT] Failed to cleanup converted PDF:', e);
        }
        
        if (code !== 0) {
          console.error('[Vision PPT] PDF to image conversion failed:', pythonStderr);
          reject(new Error(`PDF to image conversion failed: ${pythonStderr}`));
          return;
        }
        
        try {
          const result = JSON.parse(pythonStdout);
          
          if (result.error) {
            reject(new Error(result.error));
            return;
          }
          
          // Vision APIë¡œ ì´ë¯¸ì§€ ë¶„ì„
          const imagePath = result.image_path;
          
          try {
            const visionAnalysis = await analyzePDFPageImage(imagePath, 1, "general", {
              userId: options.userId,
              agentId: options.agentId,
              documentId: options.documentId
            });
            
            // ì„ì‹œ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
            try {
              fs.unlinkSync(imagePath);
            } catch (cleanupError) {
              console.error('[Vision PPT] Failed to cleanup temp image:', cleanupError);
            }
            
            console.log('[Vision PPT] Analysis completed successfully');
            resolve(visionAnalysis);
          } catch (visionError) {
            // Vision API ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try {
              fs.unlinkSync(imagePath);
            } catch (cleanupError) {
              // Ignore cleanup errors
            }
            reject(visionError);
          }
          
        } catch (error) {
          reject(new Error(`Failed to parse PDF to image output: ${error}`));
        }
      });
    });
  });
}

// Vision API: ì´ë¯¸ì§€ íŒŒì¼ ì§ì ‘ ë¶„ì„
export async function analyzeImageFile(
  imagePath: string,
  originalName?: string,
  options: {
    userId?: string;
    agentId?: number;
    documentId?: number;
  } = {}
): Promise<string | null> {
  console.log(`[Vision Image] Analyzing image file: ${originalName}`);
  
  try {
    const visionAnalysis = await analyzePDFPageImage(imagePath, 1, "general", {
      userId: options.userId,
      agentId: options.agentId,
      documentId: options.documentId
    });
    
    console.log('[Vision Image] Analysis completed successfully');
    return visionAnalysis;
  } catch (error) {
    console.error('[Vision Image] Analysis failed:', error);
    throw error;
  }
}

// Vision API: Grid ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í˜ì´ì§€ë³„ë¡œ ë§¤í•‘
function parseVisionGridResult(
  visionResult: string,
  mapping: Array<{ number: number; page: number; caption: string }>
): Map<number, string> {
  const pageDescriptions = new Map<number, string>();
  
  // ì •ê·œì‹ìœ¼ë¡œ #ë²ˆí˜¸: ì„¤ëª… íŒ¨í„´ ì¶”ì¶œ
  const pattern = /#(\d+):\s*([^#]+)/g;
  let match;
  
  while ((match = pattern.exec(visionResult)) !== null) {
    const imageNumber = parseInt(match[1], 10);
    const description = match[2].trim();
    
    // ë§¤í•‘ì—ì„œ í•´ë‹¹ ë²ˆí˜¸ì˜ í˜ì´ì§€ ì°¾ê¸°
    const mappingEntry = mapping.find(m => m.number === imageNumber);
    if (mappingEntry) {
      const page = mappingEntry.page;
      const caption = mappingEntry.caption;
      
      // ìº¡ì…˜ ì •ë³´ ì¶”ê°€
      const fullDescription = caption 
        ? `[ì´ë¯¸ì§€ ì„¤ëª…]\nìº¡ì…˜: ${caption}\në‚´ìš©: ${description}`
        : `[ì´ë¯¸ì§€ ì„¤ëª…]\n${description}`;
      
      // ê°™ì€ í˜ì´ì§€ì— ì—¬ëŸ¬ ì´ë¯¸ì§€ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëˆ„ì 
      if (pageDescriptions.has(page)) {
        const existing = pageDescriptions.get(page)!;
        pageDescriptions.set(page, `${existing}\n\n${fullDescription}`);
      } else {
        pageDescriptions.set(page, fullDescription);
      }
    }
  }
  
  console.log(`[Vision Grid Parser] Parsed ${pageDescriptions.size} pages with image descriptions`);
  return pageDescriptions;
}

// Helper: Grid ìƒì„± ë° Vision API ë¶„ì„ (PPTX/PDF ê³µí†µ)
async function processImagesWithGrid(
  extractedImages: any[],
  imageOutputDir: string,
  tempDir: string,
  options: {
    userId?: string;
    agentId?: number;
    documentId?: number;
    onProgress?: (step: string, details?: any) => void;
    isPPT?: boolean;
    convertedFilePath?: string;
  }
): Promise<{ success: boolean; visionResult?: string; pageDescriptions?: Record<number, string>; pageMapping?: any[]; error?: string }> {
  const { onProgress } = options;
  
  return new Promise(async (resolve, reject) => {
    try {
      console.log(`[Vision Grid] Processing ${extractedImages.length} images with Grid...`);
      onProgress?.('extracted', { 
        message: `âœ… 1ë‹¨ê³„: ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ (${extractedImages.length}ê°œ)`,
        totalImages: extractedImages.length,
        currentStep: 1,
        totalSteps: 4
      });
      
      // Step 2: Create image grid
      console.log('[Vision Grid] Step 2: Creating image grid...');
      onProgress?.('creating_grid', { 
        message: 'â³ 2ë‹¨ê³„: Grid ì´ë¯¸ì§€ ìƒì„± ì¤‘...',
        currentStep: 2,
        totalSteps: 4
      });
      
      const gridScript = path.join(__dirname, 'vision_processor', 'create_image_grid.py');
      const gridOutputPath = path.join(tempDir, `grid_${Date.now()}.png`);
      
      const imagePaths = extractedImages.map((img: any) => img.path);
      const imagePathsJson = JSON.stringify(imagePaths);
      const metadataJson = JSON.stringify(extractedImages);
      
      const gridPython = spawn('python3', [gridScript, imagePathsJson, gridOutputPath, metadataJson]);
      
      let gridStdout = '';
      let gridStderr = '';
      
      gridPython.stdout.on('data', (data) => {
        gridStdout += data.toString();
      });
      
      gridPython.stderr.on('data', (data) => {
        gridStderr += data.toString();
      });
      
      gridPython.on('close', async (gridCode) => {
        if (gridCode !== 0) {
          console.error('[Vision Grid] Grid creation failed:', gridStderr);
          
          // Cleanup extracted images
          try {
            fs.rmSync(imageOutputDir, { recursive: true, force: true });
          } catch (e) {}
          
          reject(new Error(`Grid creation failed: ${gridStderr}`));
          return;
        }
        
        try {
          const gridResult = JSON.parse(gridStdout);
          
          if (!gridResult.success) {
            throw new Error(gridResult.error || 'Grid creation failed');
          }
          
          console.log(`[Vision Grid] Grid created: ${gridResult.grid_size[0]}x${gridResult.grid_size[1]}px`);
          console.log(`[Vision Grid] Grid layout: ${gridResult.rows}Ã—${gridResult.cols} (${gridResult.mapping.length} images)`);
          
          // Log skipped images if any
          if (gridResult.skipped_images && gridResult.skipped_images.length > 0) {
            console.log(`[Vision Grid] âš ï¸  Skipped ${gridResult.skipped_images.length} unsupported images (WMF/EMF/etc):`);
            gridResult.skipped_images.forEach((skipped: string) => {
              console.log(`  - ${skipped}`);
            });
          }
          
          onProgress?.('grid_created', {
            message: `âœ… 2ë‹¨ê³„: Grid ìƒì„± ì™„ë£Œ (${gridResult.rows}Ã—${gridResult.cols}, ${gridResult.mapping.length}ê°œ ì´ë¯¸ì§€)`,
            gridSize: gridResult.grid_size,
            validImages: gridResult.mapping.length,
            skippedImages: gridResult.skipped_images?.length || 0,
            currentStep: 2,
            totalSteps: 4
          });
          
          // Step 3: Call Vision API with grid image
          console.log('[Vision Grid] Step 3: Analyzing grid with Vision API...');
          onProgress?.('analyzing', { 
            message: 'â³ 3ë‹¨ê³„: Vision API ë¶„ì„ ì¤‘... (ì•½ 20~30ì´ˆ ì†Œìš”)',
            currentStep: 3,
            totalSteps: 4
          });
          
          // Import analyzeImageGridWithVision function
          const { analyzeImageGridWithVision } = await import('./openai');
          
          const visionResult = await analyzeImageGridWithVision(
            gridResult.grid_path,
            gridResult.mapping,
            {
              userId: options.userId,
              agentId: options.agentId,
              documentId: options.documentId
            }
          );
          
          console.log('[Vision Grid] Vision API analysis completed');
          
          // Parse vision result and map to pages
          const pageDescriptions = parseVisionGridResult(visionResult, gridResult.mapping);
          const pagesAnalyzed = pageDescriptions.size;
          
          onProgress?.('completed', {
            message: `âœ… 4ë‹¨ê³„: ë¶„ì„ ì™„ë£Œ! (${pagesAnalyzed}í˜ì´ì§€ ì²˜ë¦¬ë¨)`,
            pagesAnalyzed: pagesAnalyzed,
            currentStep: 4,
            totalSteps: 4
          });
          
          // Cleanup temporary files
          try {
            fs.rmSync(imageOutputDir, { recursive: true, force: true });
            fs.unlinkSync(gridResult.grid_path);
            
            // If we converted .ppt file, clean it up
            if (options.isPPT && options.convertedFilePath && fs.existsSync(options.convertedFilePath)) {
              fs.unlinkSync(options.convertedFilePath);
            }
          } catch (cleanupError) {
            console.error('[Vision Grid] Cleanup failed:', cleanupError);
          }
          
          resolve({
            success: true,
            visionResult: visionResult,
            pageDescriptions: Object.fromEntries(pageDescriptions),
            pageMapping: gridResult.mapping
          });
          
        } catch (error) {
          // Cleanup on error
          try {
            fs.rmSync(imageOutputDir, { recursive: true, force: true });
            
            if (options.isPPT && options.convertedFilePath && fs.existsSync(options.convertedFilePath)) {
              fs.unlinkSync(options.convertedFilePath);
            }
          } catch (e) {}
          
          reject(error);
        }
      });
      
    } catch (error) {
      reject(error);
    }
  });
}

// Vision API: Grid ë°©ì‹ - ë¬¸ì„œì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ ì¶”ì¶œ í›„ í•œ ë²ˆì— ë¶„ì„
export async function extractAndAnalyzeImagesWithGrid(
  filePath: string,
  originalName?: string,
  options: {
    userId?: string;
    agentId?: number;
    documentId?: number;
    onProgress?: (step: string, details?: any) => void;
  } = {}
): Promise<{ success: boolean; visionResult?: string; pageDescriptions?: Record<number, string>; pageMapping?: any[]; error?: string }> {
  return new Promise(async (resolve, reject) => {
    console.log(`\n[Vision Grid] Starting image extraction and grid analysis: ${originalName}`);
    
    const { onProgress } = options;
    
    const tempDir = path.join(__dirname, 'temp_vision');
    
    try {
      // ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
      if (!fs.existsSync(tempDir)) {
        fs.mkdirSync(tempDir, { recursive: true });
      }
      
      const fileName = (originalName || '').toLowerCase();
      const isPDF = fileName.endsWith('.pdf');
      const isPPT = fileName.endsWith('.ppt');
      const isPPTX = fileName.endsWith('.pptx');
      
      let extractedImages: any[] = [];
      const imageOutputDir = path.join(tempDir, `images_${Date.now()}`);
      
      if (isPPTX || isPPT) {
        // PPTX/PPT â†’ PNG slides (ì§ì ‘ ë³€í™˜í•˜ì—¬ ë²¡í„° ê·¸ë˜í”½ í¬í•¨)
        console.log('[Vision Grid] Converting PPTX/PPT slides to PNG images...');
        onProgress?.('converting', { message: 'PPTX ìŠ¬ë¼ì´ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘...' });
        
        try {
          // ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
          if (!fs.existsSync(imageOutputDir)) {
            fs.mkdirSync(imageOutputDir, { recursive: true });
          }
          
          const convertedImages = await new Promise<string[]>((resolveConvert, rejectConvert) => {
            // LibreOfficeë¡œ PPTX ìŠ¬ë¼ì´ë“œë¥¼ PNGë¡œ ë³€í™˜
            const libreofficeCmd = `libreoffice --headless --convert-to png --outdir "${imageOutputDir}" "${filePath}"`;
            let execProcess: any;
            let hasTimedOut = false;
            
            const conversionTimeout = setTimeout(() => {
              hasTimedOut = true;
              console.error('[Vision Grid] PNG conversion timeout');
              if (execProcess) {
                execProcess.kill('SIGTERM');
              }
              rejectConvert(new Error('PNG conversion timeout (30s). Please try again.'));
            }, 30000);
            
            execProcess = exec(libreofficeCmd, (error, stdout, stderr) => {
              clearTimeout(conversionTimeout);
              
              if (hasTimedOut) {
                return;
              }
              
              if (error) {
                console.error('[Vision Grid] PNG conversion failed:', error);
                console.error('[Vision Grid] stdout:', stdout);
                console.error('[Vision Grid] stderr:', stderr);
                rejectConvert(new Error(`PNG conversion failed: ${error.message}. LibreOffice may not be available.`));
                return;
              }
              
              console.log('[Vision Grid] LibreOffice conversion completed');
              console.log('[Vision Grid] stdout:', stdout);
              
              // LibreOfficeê°€ ìƒì„±í•œ PNG íŒŒì¼ë“¤ ì°¾ê¸°
              // íŒ¨í„´: filename.png (ë‹¨ì¼), filename_1.png, filename_2.png, ... (ë³µìˆ˜)
              const baseFilename = path.basename(filePath, path.extname(filePath));
              const pngFiles: string[] = [];
              
              try {
                const files = fs.readdirSync(imageOutputDir);
                console.log('[Vision Grid] Files in output directory:', files);
                
                // PNG íŒŒì¼ë§Œ í•„í„°ë§í•˜ê³  ì •ë ¬
                const pngPattern = new RegExp(`^${baseFilename}(_\\d+)?\\.png$`, 'i');
                const matchingFiles = files
                  .filter(f => pngPattern.test(f))
                  .sort((a, b) => {
                    // íŒŒì¼ëª…ì—ì„œ ìˆ«ì ì¶”ì¶œí•˜ì—¬ ì •ë ¬
                    const numA = a.match(/_(\d+)\.png$/)?.[1];
                    const numB = b.match(/_(\d+)\.png$/)?.[1];
                    
                    if (!numA && !numB) return 0; // ë‘˜ ë‹¤ ìˆ«ì ì—†ìŒ (ë‹¨ì¼ ìŠ¬ë¼ì´ë“œ)
                    if (!numA) return -1; // aê°€ ìˆ«ì ì—†ìŒ (ë¨¼ì € ì˜´)
                    if (!numB) return 1; // bê°€ ìˆ«ì ì—†ìŒ
                    
                    return parseInt(numA) - parseInt(numB);
                  });
                
                matchingFiles.forEach(file => {
                  pngFiles.push(path.join(imageOutputDir, file));
                });
                
                if (pngFiles.length === 0) {
                  console.error('[Vision Grid] No PNG files found after conversion');
                  rejectConvert(new Error('No PNG files generated. Check LibreOffice output.'));
                  return;
                }
                
                console.log(`[Vision Grid] Successfully converted to ${pngFiles.length} PNG slides`);
                resolveConvert(pngFiles);
                
              } catch (readError) {
                console.error('[Vision Grid] Error reading output directory:', readError);
                rejectConvert(new Error(`Failed to read converted files: ${readError}`));
              }
            });
          });
          
          // PNG íŒŒì¼ë“¤ì„ extractedImages í˜•ì‹ìœ¼ë¡œ ë³€í™˜
          extractedImages = convertedImages.map((imagePath, index) => ({
            page: index + 1,
            image_index: 0,
            path: imagePath,
            bbox: [0, 0, 0, 0], // PNGëŠ” bbox ì •ë³´ ì—†ìŒ
            caption: `Slide ${index + 1}`,
            width: 0, // ì‹¤ì œ í¬ê¸°ëŠ” ë‚˜ì¤‘ì— í™•ì¸
            height: 0
          }));
          
          console.log(`[Vision Grid] Converted ${extractedImages.length} slides to PNG images`);
          
          // PPTX ë³€í™˜ ì™„ë£Œ â†’ Grid ìƒì„± ë° Vision API í˜¸ì¶œ
          const result = await processImagesWithGrid(
            extractedImages,
            imageOutputDir,
            tempDir,
            {
              ...options,
              isPPT: isPPT,
              convertedFilePath: undefined // PPTXëŠ” ë³€í™˜ íŒŒì¼ ì—†ìŒ
            }
          );
          
          resolve(result);
          
        } catch (conversionError: any) {
          console.error('[Vision Grid] PNG conversion error:', conversionError);
          reject(new Error(`PNG conversion failed: ${conversionError.message}`));
          return;
        }
        
      } else if (isPDF) {
        // PDF â†’ embedded ì´ë¯¸ì§€ ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹)
        console.log('[Vision Grid] Extracting embedded images from PDF...');
        
        const pythonScript = path.join(__dirname, 'vision_processor', 'extract_images_from_pdf.py');
        
        if (!fs.existsSync(imageOutputDir)) {
          fs.mkdirSync(imageOutputDir, { recursive: true });
        }
        
        console.log('[Vision Grid] Executing Python script:', pythonScript);
        console.log('[Vision Grid] Args:', filePath, imageOutputDir);
        const python = spawn('python3', [pythonScript, filePath, imageOutputDir]);
      
      let pythonStdout = '';
      let pythonStderr = '';
      
      python.stdout.on('data', (data) => {
        const output = data.toString();
        console.log('[Vision Python STDOUT]', output);
        pythonStdout += output;
      });
      
      python.stderr.on('data', (data) => {
        const error = data.toString();
        console.error('[Vision Python STDERR]', error);
        pythonStderr += error;
      });
      
      python.on('close', async (code) => {
        console.log('[Vision Python] Process exited with code:', code);
        console.log('[Vision Python] Full stdout:', pythonStdout);
        console.log('[Vision Python] Full stderr:', pythonStderr);
        
        if (code !== 0) {
          console.error('[Vision Grid] Image extraction failed:', pythonStderr);
          reject(new Error(`Image extraction failed: ${pythonStderr}`));
          return;
        }
        
        try {
          const extractResult = JSON.parse(pythonStdout);
          
          if (!extractResult.success || extractResult.total === 0) {
            console.log('[Vision Grid] No images found in document');
            onProgress?.('error', { message: 'ë¬¸ì„œì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' });
            resolve({ success: false, error: 'No images found in document' });
            return;
          }
          
          console.log(`[Vision Grid] Extracted ${extractResult.total} images from PDF`);
          
          // PDF ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ â†’ Grid ìƒì„± ë° Vision API í˜¸ì¶œ
          const result = await processImagesWithGrid(
            extractResult.images,
            imageOutputDir,
            tempDir,
            {
              ...options,
              isPPT: false,
              convertedFilePath: undefined
            }
          );
          
          resolve(result);
        } catch (error) {
          reject(new Error(`Failed to parse extraction output: ${error}`));
        }
      });
      
      } else {
        // Unsupported file type
        reject(new Error('Unsupported file type for grid analysis'));
        return;
      }
      
    } catch (error) {
      reject(error);
    }
  });
}

async function saveChunksToDatabase(chunks: RAGChunk[], documentId: number, agentId: number, filename?: string): Promise<void> {
  const { generateEmbedding } = await import('./openai');
  
  console.log(`\n${'â”€'.repeat(80)}`);
  console.log(`ğŸ“¦ ì²­í¬ ì €ì¥ ì‹œì‘: ${chunks.length}ê°œ ì²­í¬`);
  console.log(`${'â”€'.repeat(80)}`);
  
  // Delete existing chunks for this document
  await db.delete(agentDocumentChunks).where(eq(agentDocumentChunks.documentId, documentId));
  
  let totalEmbeddingTokens = 0;
  const startTime = Date.now();
  
  // Insert new chunks with embeddings
  for (let i = 0; i < chunks.length; i++) {
    const chunk = chunks[i];
    const chunkTokens = estimateTokens(chunk.text);
    
    // ì²­í¬ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²« 80ì)
    const preview = chunk.text.substring(0, 80).replace(/\n/g, ' ');
    const previewText = chunk.text.length > 80 ? `${preview}...` : preview;
    
    console.log(`\nğŸ“„ ì²­í¬ ${i + 1}/${chunks.length}:`);
    console.log(`   â”œâ”€ ì²­í¬ ì¸ë±ìŠ¤: ${chunk.chunk_index}`);
    console.log(`   â”œâ”€ ë‚´ìš© ê¸¸ì´: ${chunk.text.length}ì (ì˜ˆìƒ ${chunkTokens} í† í°)`);
    console.log(`   â”œâ”€ ë¯¸ë¦¬ë³´ê¸°: "${previewText}"`);
    if (chunk.keywords && chunk.keywords.length > 0) {
      console.log(`   â”œâ”€ í‚¤ì›Œë“œ: ${chunk.keywords.slice(0, 5).join(', ')}${chunk.keywords.length > 5 ? '...' : ''}`);
    }
    
    try {
      // Generate embedding for this chunk
      const embeddingStart = Date.now();
      console.log(`   â””â”€ ğŸ”„ Embedding ìƒì„± ì¤‘...`);
      const embedding = await generateEmbedding(chunk.text);
      const embeddingTime = Date.now() - embeddingStart;
      
      totalEmbeddingTokens += chunkTokens;
      
      console.log(`      âœ… Embedding ìƒì„± ì™„ë£Œ (${embeddingTime}ms)`);
      console.log(`      â””â”€ Vector í¬ê¸°: ${embedding.length} ì°¨ì›`);
      
      await db.insert(agentDocumentChunks).values({
        documentId,
        agentId,
        chunkIndex: chunk.chunk_index,
        content: chunk.text,
        keywords: chunk.keywords || [],
        metadata: chunk.metadata || {},
        embedding: embedding
      });
    } catch (error) {
      console.error(`   âš ï¸  Embedding ìƒì„± ì‹¤íŒ¨:`, error);
      // Save chunk without embedding on error
      await db.insert(agentDocumentChunks).values({
        documentId,
        agentId,
        chunkIndex: chunk.chunk_index,
        content: chunk.text,
        keywords: chunk.keywords || [],
        metadata: chunk.metadata || {}
      });
    }
  }
  
  const totalTime = Date.now() - startTime;
  const estimatedCost = (totalEmbeddingTokens / 1000000) * 0.02; // OpenAI text-embedding-3-large pricing
  
  console.log(`\n${'â”€'.repeat(80)}`);
  console.log(`âœ… ì²­í¬ ì €ì¥ ì™„ë£Œ`);
  console.log(`   â”œâ”€ ì´ ì²­í¬ ìˆ˜: ${chunks.length}ê°œ`);
  console.log(`   â”œâ”€ ì´ í† í° ìˆ˜: ~${totalEmbeddingTokens} í† í°`);
  console.log(`   â”œâ”€ ì˜ˆìƒ ë¹„ìš©: $${estimatedCost.toFixed(6)}`);
  console.log(`   â””â”€ ì†Œìš” ì‹œê°„: ${(totalTime / 1000).toFixed(1)}ì´ˆ`);
  console.log(`${'â”€'.repeat(80)}\n`);
}

function cosineSimilarity(vecA: number[], vecB: number[]): number {
  if (!vecA || !vecB || vecA.length !== vecB.length) return 0;
  
  let dotProduct = 0;
  let normA = 0;
  let normB = 0;
  
  for (let i = 0; i < vecA.length; i++) {
    dotProduct += vecA[i] * vecB[i];
    normA += vecA[i] * vecA[i];
    normB += vecB[i] * vecB[i];
  }
  
  if (normA === 0 || normB === 0) return 0;
  
  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}

function removeDuplicateSentences(text: string): string {
  const sentences = text.split(/([.!?ã€‚ï¼ï¼Ÿ\n]+)/);
  const seen = new Set<string>();
  const result: string[] = [];
  
  for (let i = 0; i < sentences.length; i += 2) {
    const sentence = sentences[i];
    const separator = sentences[i + 1] || '';
    
    if (!sentence.trim()) continue;
    
    const normalized = sentence.trim().toLowerCase().replace(/[^\wê°€-í£]/g, '');
    
    if (normalized && !seen.has(normalized)) {
      seen.add(normalized);
      result.push(sentence + separator);
    }
  }
  
  return result.join('').trim();
}

function estimateTokens(text: string): number {
  const koreanChars = (text.match(/[ê°€-í£]/g) || []).length;
  const otherChars = text.length - koreanChars;
  
  return Math.ceil(koreanChars * 0.4 + otherChars * 0.25);
}

function smartTruncate(text: string, maxLength: number): string {
  if (text.length <= maxLength) {
    return text;
  }
  
  const sentenceBoundaries = /[.!?ã€‚ï¼ï¼Ÿ\n]/g;
  
  const truncated = text.substring(0, maxLength);
  const matches = Array.from(truncated.matchAll(sentenceBoundaries));
  
  if (matches.length > 0) {
    const lastBoundary = matches[matches.length - 1];
    const boundaryPos = lastBoundary.index! + lastBoundary[0].length;
    
    if (boundaryPos >= maxLength * 0.5) {
      return text.substring(0, boundaryPos).trim();
    }
  }
  
  const ellipsis = '...';
  const truncatedForSpace = text.substring(0, maxLength - ellipsis.length);
  const lastSpace = truncatedForSpace.lastIndexOf(' ');
  
  if (lastSpace > (maxLength - ellipsis.length) * 0.7) {
    return truncatedForSpace.substring(0, lastSpace).trim() + ellipsis;
  }
  
  return truncatedForSpace.trim() + ellipsis;
}

export async function searchDocumentChunks(agentId: number, query: string, limit: number = 3): Promise<any[]> {
  try {
    console.log(`[RAG Hybrid Search] Searching for agent ${agentId}, query: "${query.substring(0, 50)}..."`);
    
    // ğŸ¯ Canon Lock: RAG ê²€ìƒ‰ ë²”ìœ„ ì œí•œìœ¼ë¡œ í† í° ì ˆê°
    const canonSettings = await storage.getAgentCanon(agentId);
    let allChunks: any[];
    
    if (canonSettings?.sources && canonSettings.sources.length > 0) {
      // Canon Lock í™œì„±í™”: SQLì—ì„œ ì§€ì •ëœ ë¬¸ì„œë§Œ ê²€ìƒ‰ (ì„±ëŠ¥ ìµœì í™”)
      const allowedDocIds = canonSettings.sources.map(s => parseInt(s)).filter(id => !isNaN(id));
      
      if (allowedDocIds.length === 0) {
        console.log(`[ğŸ”’ Canon Lock] No valid document IDs in canon sources`);
        return [];
      }
      
      console.log(`[ğŸ”’ Canon Lock] Limiting search to ${allowedDocIds.length} documents: [${allowedDocIds.join(', ')}]`);
      
      // JOIN documents to check both chunk.expiresAt and document.expiresAt
      allChunks = await db
        .select({
          id: agentDocumentChunks.id,
          documentId: agentDocumentChunks.documentId,
          agentId: agentDocumentChunks.agentId,
          chunkIndex: agentDocumentChunks.chunkIndex,
          content: agentDocumentChunks.content,
          keywords: agentDocumentChunks.keywords,
          metadata: agentDocumentChunks.metadata,
          embedding: agentDocumentChunks.embedding,
          createdAt: agentDocumentChunks.createdAt,
          expiresAt: agentDocumentChunks.expiresAt
        })
        .from(agentDocumentChunks)
        .innerJoin(documents, eq(agentDocumentChunks.documentId, documents.id))
        .where(
          and(
            eq(agentDocumentChunks.agentId, agentId),
            inArray(agentDocumentChunks.documentId, allowedDocIds),
            // ì²­í¬ ë˜ëŠ” ë¬¸ì„œ ë ˆë²¨ì—ì„œ ë§Œë£Œ í™•ì¸ (ë‘˜ ë‹¤)
            or(
              isNull(agentDocumentChunks.expiresAt),
              gt(agentDocumentChunks.expiresAt, new Date())
            ),
            or(
              isNull(documents.expiresAt),
              gt(documents.expiresAt, new Date())
            )
          )
        );
      
      console.log(`[ğŸ”’ Canon Lock] Found ${allChunks.length} chunks from canon sources (ë§Œë£Œë˜ì§€ ì•ŠìŒ)`);
    } else {
      // Canon Lock ë¹„í™œì„±í™”: ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰ (ë§Œë£Œë˜ì§€ ì•Šì€ ê²ƒë§Œ)
      console.log(`[RAG Search] Canon Lock disabled - searching all documents`);
      allChunks = await db
        .select({
          id: agentDocumentChunks.id,
          documentId: agentDocumentChunks.documentId,
          agentId: agentDocumentChunks.agentId,
          chunkIndex: agentDocumentChunks.chunkIndex,
          content: agentDocumentChunks.content,
          keywords: agentDocumentChunks.keywords,
          metadata: agentDocumentChunks.metadata,
          embedding: agentDocumentChunks.embedding,
          createdAt: agentDocumentChunks.createdAt,
          expiresAt: agentDocumentChunks.expiresAt
        })
        .from(agentDocumentChunks)
        .innerJoin(documents, eq(agentDocumentChunks.documentId, documents.id))
        .where(
          and(
            eq(agentDocumentChunks.agentId, agentId),
            // ì²­í¬ ë˜ëŠ” ë¬¸ì„œ ë ˆë²¨ì—ì„œ ë§Œë£Œ í™•ì¸ (ë‘˜ ë‹¤)
            or(
              isNull(agentDocumentChunks.expiresAt),
              gt(agentDocumentChunks.expiresAt, new Date())
            ),
            or(
              isNull(documents.expiresAt),
              gt(documents.expiresAt, new Date())
            )
          )
        );
    }
    
    if (allChunks.length === 0) {
      console.log('[RAG Hybrid Search] No chunks found for this agent');
      return [];
    }
    
    console.log(`[RAG Hybrid Search] Found ${allChunks.length} chunks total`);
    
    // Generate query embedding for semantic search
    let queryEmbedding: number[] | null = null;
    try {
      const { generateEmbedding } = await import('./openai');
      queryEmbedding = await generateEmbedding(query);
      console.log('[RAG Hybrid Search] Query embedding generated successfully');
    } catch (error) {
      console.error('[RAG Hybrid Search] Failed to generate query embedding:', error);
    }
    
    // Prepare keyword search
    const queryKeywords = query.toLowerCase().split(/\s+/).filter(w => w.length > 2);
    
    // Score chunks using hybrid approach
    const scoredChunks = allChunks.map((chunk: any) => {
      const content = chunk.content.toLowerCase();
      const chunkKeywords = (chunk.keywords as string[]) || [];
      
      // 1. Keyword-based score (0-10 points)
      let keywordScore = 0;
      for (const keyword of queryKeywords) {
        if (content.includes(keyword)) keywordScore += 2;
        if (chunkKeywords.some(k => k.toLowerCase().includes(keyword))) keywordScore += 3;
      }
      
      // 2. Semantic similarity score (0-10 points, scaled from 0-1)
      let semanticScore = 0;
      if (queryEmbedding && chunk.embedding) {
        try {
          const embedding = typeof chunk.embedding === 'string' 
            ? JSON.parse(chunk.embedding) 
            : chunk.embedding;
          const similarity = cosineSimilarity(queryEmbedding, embedding);
          semanticScore = similarity * 10; // Scale to 0-10
        } catch (error) {
          console.error('[RAG Hybrid Search] Error calculating similarity:', error);
        }
      }
      
      // Combined score: keyword (40%) + semantic (60%)
      const totalScore = (keywordScore * 0.4) + (semanticScore * 0.6);
      
      return { 
        ...chunk, 
        score: totalScore,
        keywordScore,
        semanticScore
      };
    });
    
    // Sort by total score
    const sortedChunks = scoredChunks
      .filter((c: any) => c.score > 0.1)
      .sort((a: any, b: any) => b.score - a.score);
    
    // Token-aware re-ranking: select chunks within token budget
    const TOKEN_BUDGET = 8000;
    const SYSTEM_PROMPT_TOKENS = 2000;
    const DIALOGUE_TOKENS = 1000;
    const REMAINING_BUDGET = TOKEN_BUDGET - SYSTEM_PROMPT_TOKENS - DIALOGUE_TOKENS;
    
    const selectedChunks: any[] = [];
    let totalTokens = 0;
    
    for (const chunk of sortedChunks) {
      if (selectedChunks.length >= 5) break;
      
      const chunkTokens = estimateTokens(chunk.content);
      
      if (totalTokens + chunkTokens <= REMAINING_BUDGET) {
        selectedChunks.push(chunk);
        totalTokens += chunkTokens;
      }
    }
    
    const finalCount = Math.max(2, Math.min(selectedChunks.length, 5));
    const topChunks = selectedChunks.slice(0, finalCount);
    
    console.log(`[RAG Token Budget] Budget: ${REMAINING_BUDGET}, Used: ${totalTokens}, Selected: ${topChunks.length} chunks`);
    
    // Optimize chunks: remove duplicates + smart truncate
    const MAX_CHUNK_LENGTH = 400;
    const optimizedChunks = topChunks.map((chunk: any) => {
      const originalLength = chunk.content.length;
      
      const deduplicated = removeDuplicateSentences(chunk.content);
      const dedupeReduction = originalLength - deduplicated.length;
      
      const truncated = deduplicated.length > MAX_CHUNK_LENGTH 
        ? smartTruncate(deduplicated, MAX_CHUNK_LENGTH)
        : deduplicated;
      
      const finalTokens = estimateTokens(truncated);
      
      return {
        ...chunk,
        content: truncated,
        _originalLength: originalLength,
        _dedupedLength: deduplicated.length,
        _finalLength: truncated.length,
        _dedupeReduction: dedupeReduction,
        _totalReduction: originalLength - truncated.length,
        _estimatedTokens: finalTokens
      };
    });
    
    const totalEstimatedTokens = optimizedChunks.reduce((sum, c) => sum + c._estimatedTokens, 0);
    
    console.log(`[RAG Optimization] ${optimizedChunks.length} chunks selected:`);
    optimizedChunks.forEach((chunk: any, idx: number) => {
      console.log(`  ${idx + 1}. Score: ${chunk.score.toFixed(2)} (kw: ${chunk.keywordScore.toFixed(2)}, sem: ${chunk.semanticScore.toFixed(2)})`);
      console.log(`     Reduction: ${chunk._originalLength} â†’ ${chunk._dedupedLength} (dedupe -${chunk._dedupeReduction}) â†’ ${chunk._finalLength} (truncate) = ${chunk._totalReduction} chars saved`);
      console.log(`     Est. tokens: ${chunk._estimatedTokens}`);
    });
    console.log(`[RAG Total] Estimated tokens for all chunks: ${totalEstimatedTokens}`);
    
    return optimizedChunks;
    
  } catch (error) {
    console.error('[RAG Hybrid Search] Error:', error);
    return [];
  }
}

interface VisionAnalysis {
  diagramCount: number;
  recommendVision: boolean;
  recommendationLevel: 'unnecessary' | 'optional' | 'recommended' | 'highly_recommended';
  visionScore: number;
  estimatedCost: number;
  hasVisionProcessed: boolean;
  reasons: string[];
  benefits?: string[];
}

export async function analyzeDocumentStructure(
  text: string,
  metadata: any
): Promise<VisionAnalysis> {
  console.log(`\n${'â”€'.repeat(80)}`);
  console.log('ğŸ“Š Vision API ë¶„ì„ ì‹œì‘');
  console.log(`${'â”€'.repeat(80)}`);
  
  let diagramCount = 0;
  let visionScore = 0;
  const reasons: string[] = [];
  
  const textLength = text.length;
  const pageCount = metadata?.total_pages || 1;
  const ocrUsed = metadata?.ocr_used || false;
  const ocrPages = metadata?.ocr_pages || 0;
  
  console.log(`\nğŸ“„ ë¬¸ì„œ ê¸°ë³¸ ì •ë³´:`);
  console.log(`   â”œâ”€ í…ìŠ¤íŠ¸ ê¸¸ì´: ${textLength.toLocaleString()}ì`);
  console.log(`   â”œâ”€ í˜ì´ì§€ ìˆ˜: ${pageCount}í˜ì´ì§€`);
  console.log(`   â””â”€ OCR ì‚¬ìš©: ${ocrUsed ? `Yes (${ocrPages}í˜ì´ì§€)` : 'No'}`);
  
  const avgCharsPerPage = textLength / pageCount;
  
  console.log(`\nğŸ“ í˜ì´ì§€ ë°€ë„ ë¶„ì„:`);
  console.log(`   â””â”€ í‰ê·  í˜ì´ì§€ë‹¹ ë¬¸ì ìˆ˜: ${Math.round(avgCharsPerPage)}ì`);
  
  if (avgCharsPerPage < 300) {
    diagramCount = Math.floor(pageCount * 0.7);
    visionScore += 8;
    const reason = `í˜ì´ì§€ë‹¹ í…ìŠ¤íŠ¸ ì ìŒ (${Math.round(avgCharsPerPage)}ì) - ë‹¤ì´ì–´ê·¸ë¨/ì°¨íŠ¸ ê°€ëŠ¥ì„± ë†’ìŒ`;
    reasons.push(reason);
    console.log(`      âœ… ${reason} â†’ +8ì `);
  } else if (avgCharsPerPage < 600) {
    diagramCount = Math.floor(pageCount * 0.4);
    visionScore += 5;
    const reason = `í˜ì´ì§€ë‹¹ í…ìŠ¤íŠ¸ ì¤‘ê°„ (${Math.round(avgCharsPerPage)}ì) - ì¼ë¶€ ì‹œê° ìë£Œ í¬í•¨ ê°€ëŠ¥`;
    reasons.push(reason);
    console.log(`      âœ… ${reason} â†’ +5ì `);
  } else {
    console.log(`      â„¹ï¸  í…ìŠ¤íŠ¸ ë°€ë„ ë†’ìŒ (${Math.round(avgCharsPerPage)}ì) - ì‹œê° ìë£Œ ê°€ëŠ¥ì„± ë‚®ìŒ`);
  }
  
  // ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì •ì˜ (ìš°ì„ ìˆœìœ„ ë° ì ìˆ˜ ê°€ì¤‘ì¹˜ í¬í•¨)
  const keywordCategories = {
    // ì§€ë„/ë…¸ì„ ë„ (ê°€ì¤‘ì¹˜ ë†’ìŒ: 8ì )
    maps: {
      keywords: ['metro', 'subway', 'map', 'route', 'station', 'line', 'transfer', 'network', 
                 'ì§€í•˜ì² ', 'ë…¸ì„ ', 'ì—­', 'í™˜ìŠ¹', 'ì§€ë„', 'ê²½ë¡œ'],
      weight: 8,
      description: 'ì§€ë„/ë…¸ì„ ë„'
    },
    // ë³µì¡í•œ ìˆ˜ì‹/ë¬¼ë¦¬í•™ (ê°€ì¤‘ì¹˜ ë†’ìŒ: 8ì ) - ìˆ˜ì‹ì€ OCRë¡œ ì½ê¸° ì–´ë ¤ì›€
    equations: {
      keywords: ['equation', 'formula', 'mathematical', 'ìˆ˜ì‹', 'ê³µì‹', 'âˆ‘', 'âˆ«', 'âˆš', 'âˆ‚', 'âˆ', 
                 'Â±', 'â‰¤', 'â‰¥', 'â‰ ', 'âˆˆ', 'âˆ€', 'âˆƒ', 'theorem', 'ì •ë¦¬', 'coulomb', 'force', 'law',
                 'vector', 'field', 'charge', 'electric', 'magnetic', 'ì „ê¸°ì¥', 'ì¿¨ë¡±', 'ì „í•˜', 
                 'ë²¡í„°', 'potential', 'energy', 'ì—ë„ˆì§€', 'momentum', 'velocity', 'ì†ë„', 
                 'acceleration', 'ê°€ì†ë„', 'derivative', 'integral', 'ë¯¸ë¶„', 'ì ë¶„', 'limit',
                 'Ï€', 'Îµ', 'Î¼', 'Ïƒ', 'Î©', 'Î”', 'Î»', 'Î¸', 'Ï†', 'Ï‰'],
      weight: 8,
      description: 'ìˆ˜í•™/ë¬¼ë¦¬ ìˆ˜ì‹'
    },
    // íšŒë¡œë„/ì „ìê³µí•™ (ê°€ì¤‘ì¹˜ ë†’ìŒ: 7ì ) - íšŒë¡œë„ëŠ” í…ìŠ¤íŠ¸ë¡œ í‘œí˜„ ë¶ˆê°€
    circuits: {
      keywords: ['circuit', 'resistor', 'capacitor', 'inductor', 'voltage', 'current', 'ohm',
                 'semiconductor', 'transistor', 'diode', 'amplifier', 'íšŒë¡œ', 'ì €í•­', 'ì „ë¥˜', 
                 'ì „ì••', 'ë°˜ë„ì²´', 'ë‹¤ì´ì˜¤ë“œ', 'íŠ¸ëœì§€ìŠ¤í„°', 'ì¦í­ê¸°', 'schematic', 'íšŒë¡œë„',
                 'wiring', 'connection', 'node', 'ë…¸ë“œ', 'ground', 'GND', 'VCC', 'VDD'],
      weight: 7,
      description: 'íšŒë¡œë„/ì „ìê³µí•™'
    },
    // ë³µì¡í•œ í‘œ (ê°€ì¤‘ì¹˜ ë†’ìŒ: 6ì )
    tables: {
      keywords: ['table', 'matrix', 'grid', 'cell', 'row', 'column', 'í‘œ', 'í–‰', 'ì—´', 'ì…€', 
                 'thead', 'tbody', '|---', 'â”Œ', 'â””', 'â”œ'],
      weight: 6,
      description: 'ë³µì¡í•œ í‘œ/í–‰ë ¬'
    },
    // ë‹¤ì´ì–´ê·¸ë¨/ì°¨íŠ¸ (ê°€ì¤‘ì¹˜ ì¤‘ê°„: 5ì )
    diagrams: {
      keywords: ['ê·¸ë¦¼', 'ë„í‘œ', 'figure', 'chart', 'graph', 'diagram', 'flowchart', 'plot',
                 'visualization', 'ì‹œê°í™”', 'ì°¨íŠ¸', 'ê·¸ë˜í”„', 'illustration', 'ë„í•´'],
      weight: 5,
      description: 'ë‹¤ì´ì–´ê·¸ë¨/ì°¨íŠ¸'
    }
  };
  
  // ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ì ìˆ˜ ê³„ì‚°
  console.log(`\nğŸ” ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë¶„ì„:`);
  let totalKeywordCount = 0;
  const foundCategories: string[] = [];
  
  for (const [category, config] of Object.entries(keywordCategories)) {
    const foundKeywords = config.keywords.filter(kw => {
      const lowerText = text.toLowerCase();
      return lowerText.includes(kw.toLowerCase());
    });
    
    if (foundKeywords.length > 0) {
      const keywordCount = foundKeywords.reduce((sum, kw) => {
        const regex = new RegExp(kw.replace(/[.*+?^${}()|[\]\\]/g, '\\$&'), 'gi');
        return sum + (text.match(regex) || []).length;
      }, 0);
      
      totalKeywordCount += keywordCount;
      const categoryScore = Math.min(config.weight, keywordCount * 0.8);
      visionScore += categoryScore;
      diagramCount += Math.floor(keywordCount * 0.6);
      
      foundCategories.push(config.description);
      const reason = `${config.description} ê°ì§€: ${foundKeywords.length}ê°œ í‚¤ì›Œë“œ (${keywordCount}íšŒ ì–¸ê¸‰) â†’ +${categoryScore.toFixed(1)}ì `;
      reasons.push(reason);
      
      console.log(`   â”œâ”€ ${config.description}:`);
      console.log(`   â”‚  â”œâ”€ ë°œê²¬ëœ í‚¤ì›Œë“œ: ${foundKeywords.slice(0, 8).join(', ')}${foundKeywords.length > 8 ? '...' : ''}`);
      console.log(`   â”‚  â”œâ”€ ì´ ì–¸ê¸‰ íšŸìˆ˜: ${keywordCount}íšŒ`);
      console.log(`   â”‚  â””â”€ ì ìˆ˜: +${categoryScore.toFixed(1)}ì  (ìµœëŒ€ ${config.weight}ì )`);
    }
  }
  
  if (totalKeywordCount === 0) {
    console.log(`   â””â”€ â„¹ï¸  íŠ¹ë³„í•œ í‚¤ì›Œë“œ ê°ì§€ë˜ì§€ ì•ŠìŒ`);
  }
  
  if (ocrUsed && ocrPages > 0) {
    visionScore += 6;
    const reason = `OCR ì‚¬ìš©ë¨ (${ocrPages}í˜ì´ì§€) - ìŠ¤ìº” ë¬¸ì„œë¡œ ì‹œê° ìë£Œ í¬í•¨ ê°€ëŠ¥ì„± ë†’ìŒ`;
    reasons.push(reason);
    console.log(`\nğŸ“· OCR ì •ë³´:`);
    console.log(`   â””â”€ ${reason} â†’ +6ì `);
  }
  
  visionScore = Math.min(10, visionScore);
  const recommendVision = visionScore >= 5;
  
  // ì¶”ì²œ ë ˆë²¨ ë¶„ë¥˜ (0-3: ë¶ˆí•„ìš”, 4-6: ì„ íƒì , 7-9: ì¶”ì²œ, 10+: ì ê·¹ ì¶”ì²œ)
  let recommendationLevel: 'unnecessary' | 'optional' | 'recommended' | 'highly_recommended';
  if (visionScore >= 10) {
    recommendationLevel = 'highly_recommended';
  } else if (visionScore >= 7) {
    recommendationLevel = 'recommended';
  } else if (visionScore >= 4) {
    recommendationLevel = 'optional';
  } else {
    recommendationLevel = 'unnecessary';
  }
  
  const estimatedCost = diagramCount * 0.00255;
  
  // ìµœì¢… ê²°ê³¼ í‘œì‹œ
  console.log(`\n${'â”€'.repeat(80)}`);
  console.log(`ğŸ“Š Vision API ë¶„ì„ ê²°ê³¼`);
  console.log(`${'â”€'.repeat(80)}`);
  console.log(`\nğŸ¯ ìµœì¢… ì ìˆ˜: ${visionScore}/10`);
  console.log(`   â”œâ”€ ì¶”ì • ë‹¤ì´ì–´ê·¸ë¨: ${diagramCount}ê°œ`);
  console.log(`   â”œâ”€ ì˜ˆìƒ ë¹„ìš©: $${estimatedCost.toFixed(4)} (Sharp ìµœì í™” ì ìš©)`);
  console.log(`   â”œâ”€ Vision API ê¶Œì¥: ${recommendVision ? 'âœ… YES' : 'âŒ NO'}`);
  console.log(`   â””â”€ ì¶”ì²œ ë ˆë²¨: ${recommendationLevel}`);
  
  if (visionScore >= 10) {
    console.log(`\nğŸ”¥ ì ê·¹ ì¶”ì²œ: ì§€ë„/ë…¸ì„ ë„/ë³µì¡í•œ ìˆ˜ì‹/í‘œ ë“±ì´ í¬í•¨ë˜ì–´ Vision API í•„ìˆ˜`);
  } else if (visionScore >= 7) {
    console.log(`\nâš ï¸  ì¶”ì²œ: ì‹œê° ìë£Œê°€ ë§ì•„ Vision API ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤`);
  } else if (visionScore >= 4) {
    console.log(`\nâ„¹ï¸  ì„ íƒì : ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œë¡œ ì¶©ë¶„í•˜ì§€ë§Œ, í•„ìš”ì‹œ Vision API ì‚¬ìš© ê°€ëŠ¥`);
  } else {
    console.log(`\nâœ… ë¶ˆí•„ìš”: í…ìŠ¤íŠ¸ ë¬¸ì„œë¡œ Vision APIê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤`);
  }
  
  if (reasons.length > 0) {
    console.log(`\nğŸ“‹ ê°ì§€ëœ ë‚´ìš©:`);
    reasons.forEach(r => console.log(`   â€¢ ${r}`));
  }
  
  console.log(`${'â”€'.repeat(80)}\n`);
  
  return {
    diagramCount,
    recommendVision,
    recommendationLevel,
    visionScore,
    estimatedCost,
    hasVisionProcessed: false,
    reasons
  };
}

export async function optimizeImageForVision(imagePath: string): Promise<Buffer> {
  const sharp = (await import('sharp')).default;
  
  console.log(`[Sharp Optimization] ì´ë¯¸ì§€ ìµœì í™” ì‹œì‘: ${imagePath}`);
  
  const image = sharp(imagePath);
  const metadata = await image.metadata();
  
  console.log(`  - ì›ë³¸ í¬ê¸°: ${metadata.width}Ã—${metadata.height}px`);
  console.log(`  - ì›ë³¸ í¬ë§·: ${metadata.format}`);
  
  const optimized = await image
    .resize(512, 512, {
      fit: 'inside',
      withoutEnlargement: false
    })
    .png()
    .toBuffer();
  
  console.log(`  - ìµœì í™” ì™„ë£Œ: 512Ã—512px (max)`);
  console.log(`  - íŒŒì¼ í¬ê¸°: ${(optimized.length / 1024).toFixed(2)} KB`);
  
  return optimized;
}
