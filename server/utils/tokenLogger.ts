import { storage } from '../storage';
import type { InsertTokenUsage } from '@shared/schema';

// GPT ëª¨ë¸ë³„ ë¹„ìš© (USD per 1K tokens)
const MODEL_COSTS = {
  'gpt-4o': { prompt: 0.0025, completion: 0.01 },
  'gpt-4o-mini': { prompt: 0.00015, completion: 0.0006 },
  'gpt-4-turbo': { prompt: 0.01, completion: 0.03 },
  'gpt-4': { prompt: 0.03, completion: 0.06 },
  'gpt-3.5-turbo': { prompt: 0.0005, completion: 0.0015 },
  'o1-preview': { prompt: 0.015, completion: 0.06 },
  'o1-mini': { prompt: 0.003, completion: 0.012 },
} as const;

// ë¹„ìš© ê³„ì‚°
export function calculateCost(model: string, promptTokens: number, completionTokens: number): number {
  const costs = MODEL_COSTS[model as keyof typeof MODEL_COSTS] || MODEL_COSTS['gpt-4o-mini'];
  const promptCost = (promptTokens / 1000) * costs.prompt;
  const completionCost = (completionTokens / 1000) * costs.completion;
  return promptCost + completionCost;
}

// í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…
export async function logTokenUsage(params: {
  userId?: string;
  agentId?: number;
  conversationId?: number;
  groupChatId?: number;
  feature: string;
  model: string;
  promptTokens: number;
  completionTokens: number;
  requestDuration?: number;
  metadata?: any;
}): Promise<void> {
  try {
    const totalTokens = params.promptTokens + params.completionTokens;
    const estimatedCost = calculateCost(params.model, params.promptTokens, params.completionTokens);

    const tokenUsageData: InsertTokenUsage = {
      userId: params.userId || null,
      agentId: params.agentId || null,
      conversationId: params.conversationId || null,
      groupChatId: params.groupChatId || null,
      feature: params.feature,
      model: params.model,
      promptTokens: params.promptTokens,
      completionTokens: params.completionTokens,
      totalTokens,
      estimatedCost: estimatedCost.toString(),
      requestDuration: params.requestDuration || null,
      metadata: params.metadata ? JSON.stringify(params.metadata) : null,
    };

    await storage.logTokenUsage(tokenUsageData);
  } catch (error) {
    console.error('[ğŸ”¥ í† í° ë¡œê¹… ì‹¤íŒ¨]:', error);
  }
}

// OpenAI API ì‘ë‹µì—ì„œ í† í° ì •ë³´ ì¶”ì¶œ ë° ë¡œê¹…
export async function logOpenAIUsage(
  response: any,
  params: {
    userId?: string;
    agentId?: number;
    conversationId?: number;
    groupChatId?: number;
    feature: string;
    requestStartTime?: number;
    metadata?: any;
  }
): Promise<void> {
  try {
    if (!response?.usage) {
      console.warn('[âš ï¸ í† í° ì •ë³´ ì—†ìŒ] OpenAI ì‘ë‹µì— usage ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤');
      return;
    }

    const { prompt_tokens, completion_tokens } = response.usage;
    const model = response.model || 'gpt-4o-mini';
    const requestDuration = params.requestStartTime ? Date.now() - params.requestStartTime : undefined;

    await logTokenUsage({
      ...params,
      model,
      promptTokens: prompt_tokens,
      completionTokens: completion_tokens,
      requestDuration,
    });
  } catch (error) {
    console.error('[ğŸ”¥ OpenAI í† í° ë¡œê¹… ì‹¤íŒ¨]:', error);
  }
}
